import OpenAI from 'openai';

// Removed: import { nodeManager } from '../NodeManager'
import { ImageProcessor } from '../../utils/ImageProcessor';

import type { NodeDefinition } from '@comfytavern/types'
// æœ¬åœ°ç±»å‹å®šä¹‰
interface APISettings {
  use_env_vars: boolean;
  base_url: string;
  api_key: string;
}

interface CustomContentPart {
  type: 'text' | 'image_url';
  text?: string;
  image_url?: {
    url: string;
  };
}

interface CustomMessage {
  role: 'user' | 'assistant' | 'system';
  content: string | CustomContentPart[];
}

function convertToApiMessage(msg: CustomMessage): OpenAI.Chat.ChatCompletionMessageParam {
  const content = typeof msg.content === 'string'
    ? msg.content
    : msg.content.map(part => {
        if (part.type === 'text') {
          return { type: 'text' as const, text: part.text || '' };
        }
        return { type: 'image_url' as const, image_url: part.image_url! };
      });

  switch (msg.role) {
    case 'system':
      return { role: 'system', content: msg.content as string }; // System message content must be a string
    case 'user':
      return { role: 'user', content };
    case 'assistant':
      // Assistant message content can be string or null, but here we assume string from our logic
      return { role: 'assistant', content: msg.content as string | null };
    default:
      // Fallback for safety, though should not be reached with CustomMessage type
      return { role: 'user', content };
  }
}

export class OpenAIChatNodeImpl {
  private static formatConversation(messages: CustomMessage[]): string {
    let formatted = `=== å®Œæ•´å¯¹è¯å†å² ===\n\n`
    
    for (const msg of messages) {
      const role = msg.role.charAt(0).toUpperCase() + msg.role.slice(1)
      let content = ''

      if (Array.isArray(msg.content)) {
        const textContent = msg.content.find((part: CustomContentPart) => 
          part.type === 'text' && part.text !== undefined
        )
        content = textContent?.text ? `${textContent.text} [åŒ…å«å›¾åƒ]` : '[åŒ…å«å›¾åƒ]'
      } else {
        content = msg.content
      }
      
      formatted += `${role}: ${content}\n`
      formatted += '-'.repeat(40) + '\n'
    }
    
    return formatted
  }

  static async execute(inputs: Record<string, any>): Promise<Record<string, any>> {
    const {
      api_settings,
      model,
      temperature,
      max_tokens,
      system_prompt,
      clear_history,
      user_input,
      external_history,
      image
    } = inputs

    const client = new OpenAI({
      baseURL: api_settings.base_url,
      apiKey: api_settings.api_key
    })

    // å¤„ç†å¤–éƒ¨å†å²
    let external_messages: CustomMessage[] = []
    if (external_history) {
      try {
        external_messages = JSON.parse(external_history)
      } catch (error) {
        return {
          full_conversation: 'Error: Invalid external history format.',
          current_output: 'Error: Invalid external history format.',
          history: JSON.stringify([])
        }
      }
    }

    // å†…éƒ¨å†å²ï¼ˆç®€åŒ–å¤„ç†ï¼Œç§»é™¤ç¼“å­˜ï¼‰
    let internal_history: CustomMessage[] = clear_history ? [] : []

    // æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    const messages: CustomMessage[] = []
    if (system_prompt) {
      messages.push({ role: 'system', content: system_prompt })
    }
    messages.push(...external_messages)
    messages.push(...internal_history)

    // æ·»åŠ ç”¨æˆ·è¾“å…¥
    let new_message: CustomMessage
    if (image) {
      try {
        const image_content = await ImageProcessor.encodeImage(image)
        new_message = {
          role: 'user',
          content: [
            { type: 'text', text: user_input },
            {
              type: 'image_url',
              image_url: { url: `data:image/jpeg;base64,${image_content}` }
            }
          ]
        }
      } catch (error) {
        const error_message = `Error encoding image: ${error instanceof Error ? error.message : String(error)}`
        return {
          full_conversation: this.formatConversation(messages),
          current_output: error_message,
          history: JSON.stringify(messages)
        }
      }
    } else {
      new_message = { role: 'user', content: user_input }
    }

    messages.push(new_message)
    internal_history.push(new_message)

    try {
      const completion = await client.chat.completions.create({
        model,
        messages: messages.map(msg => convertToApiMessage(msg)),
        temperature,
        max_tokens
      })

      const assistant_response = completion.choices[0]?.message?.content?.trim() || ''

      if (!assistant_response) {
        return {
          full_conversation: this.formatConversation(messages),
          current_output: 'APIè¿”å›äº†ç©ºå›å¤ï¼Œæœªå†™å…¥å†å²è®°å½•ã€‚',
          history: JSON.stringify(external_messages.concat(internal_history))
        }
      }

      const assistant_message: CustomMessage = {
        role: 'assistant',
        content: assistant_response
      }
      internal_history.push(assistant_message)

      return {
        full_conversation: this.formatConversation(messages.concat([assistant_message])),
        current_output: assistant_response,
        history: JSON.stringify(external_messages.concat(internal_history))
      }
    } catch (error) {
      const error_message = `API Error: ${error instanceof Error ? error.message : String(error)}`
      return {
        full_conversation: this.formatConversation(messages),
        current_output: error_message,
        history: JSON.stringify(external_messages.concat(internal_history))
      }
    }
  }
}

// Renamed export to 'definition'
export const definition: NodeDefinition = {
  type: 'OpenAIChat', // Base type name
  // namespace will be set via index.ts registerer (e.g., 'builtin')
  category: 'TEST-LLM', // Functional category
  displayName: 'ğŸ¦‰OpenAIè¿ç»­èŠå¤©',
  description: 'ä¸OpenAIæ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œæ”¯æŒå†å²è®°å½•å’Œå›¾åƒè¾“å…¥',

  inputs: {
    api_settings: {
      dataFlowType: 'OBJECT', // API_SETTINGS maps to OBJECT
      displayName: 'APIé…ç½®',
      description: 'OpenAI API çš„ç›¸å…³é…ç½®',
      required: true,
      matchCategories: ['LlmConfig']
    },
    model: {
      dataFlowType: 'STRING',
      displayName: 'æ¨¡å‹åç§°',
      description: 'ç”¨äºç”Ÿæˆå¯¹è¯çš„æ¨¡å‹åç§°',
      required: true,
      config: {
        default: 'gpt-4o',
        placeholder: 'è¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ gpt-4o, gemini-1.5-flash-exp-0827 ç­‰'
      }
    },
    temperature: {
      dataFlowType: 'FLOAT',
      displayName: 'æ¸©åº¦å‚æ•°',
      description: 'æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœº',
      required: true,
      config: {
        default: 0.7,
        min: 0,
        max: 2,
        step: 0.1
      }
    },
    max_tokens: {
      dataFlowType: 'INTEGER',
      displayName: 'æœ€å¤§ä»¤ç‰Œæ•°',
      description: 'é™åˆ¶æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦',
      required: true,
      config: {
        default: 512,
        min: 1,
        max: 16384
      }
    },
    system_prompt: {
      dataFlowType: 'STRING',
      displayName: 'ç³»ç»Ÿæç¤º',
      description: 'ç”¨äºæŒ‡å¯¼æ¨¡å‹è¡Œä¸ºçš„åˆå§‹æç¤º',
      required: true,
      matchCategories: ['Prompt'],
      config: {
        multiline: true
      }
    },
    clear_history: {
      dataFlowType: 'BOOLEAN',
      displayName: 'æ¸…é™¤å†å²è®°å½•',
      description: 'æ˜¯å¦åœ¨æ¯æ¬¡è¿è¡Œæ—¶æ¸…é™¤å¯¹è¯å†å²',
      required: true,
      config: {
        default: false
      }
    },
    user_input: {
      dataFlowType: 'STRING',
      displayName: 'ç”¨æˆ·è¾“å…¥',
      description: 'ç”¨æˆ·å‘é€ç»™æ¨¡å‹çš„æ–‡æœ¬',
      required: true,
      matchCategories: ['Prompt'],
      config: {
        multiline: true
      }
    },
    external_history: {
      dataFlowType: 'STRING', // HISTORY input is a JSON string
      displayName: 'å¤–éƒ¨å†å²è®°å½•',
      description: 'ä»å¤–éƒ¨ä¼ å…¥çš„å†å²å¯¹è¯è®°å½• (JSONå­—ç¬¦ä¸²)',
      required: false,
      matchCategories: ['ChatHistory', 'Json']
    },
    image: {
      dataFlowType: 'STRING', // IMAGE input is URL/Base64 string
      displayName: 'å›¾åƒè¾“å…¥',
      description: 'ç”¨æˆ·ä¸Šä¼ çš„å›¾åƒ (URL/Base64)',
      required: false,
      matchCategories: ['ImageData', 'Url']
    }
  },

  outputs: {
    full_conversation: {
      dataFlowType: 'STRING',
      displayName: 'å®Œæ•´å¯¹è¯å†å²',
      description: 'åŒ…å«ç³»ç»Ÿæç¤ºã€ç”¨æˆ·è¾“å…¥å’Œæ¨¡å‹å›å¤çš„å®Œæ•´å¯¹è¯è®°å½•',
      matchCategories: ['ChatHistory']
    },
    current_output: {
      dataFlowType: 'STRING',
      displayName: 'å½“å‰è¾“å‡º',
      description: 'æ¨¡å‹ç”Ÿæˆçš„å½“å‰å›å¤',
      matchCategories: ['LlmOutput']
    },
    history: {
      dataFlowType: 'STRING', // HISTORY output is a JSON string
      displayName: 'æ›´æ–°åçš„å†å²è®°å½•',
      description: 'åŒ…å«æ‰€æœ‰å¯¹è¯è½®æ¬¡çš„å†å²è®°å½•ï¼Œç”¨äºåç»­å¯¹è¯ (JSONå­—ç¬¦ä¸²)',
      matchCategories: ['ChatHistory', 'Json']
    }
  },

  execute: OpenAIChatNodeImpl.execute
}

// Removed: Node registration is now handled by index.ts