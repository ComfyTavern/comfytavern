import OpenAI from 'openai';

import { ImageProcessor } from '../../utils/ImageProcessor';

import type { NodeDefinition, ChunkPayload } from '@comfytavern/types'; // å¯¼å…¥ ChunkPayload

// æœ¬åœ°ç±»å‹å®šä¹‰
interface APISettings {
  use_env_vars: boolean;
  base_url: string;
  api_key: string;
}
// Removed: import { nodeManager } from '../NodeManager'

export class OpenAINodeImpl {
  static async *execute(
    inputs: Record<string, any>
  ): AsyncGenerator<ChunkPayload, Record<string, any> | void, undefined> {
    const {
      api_settings,
      model,
      prompt,
      temperature,
      max_tokens,
      system_prompt,
      history,
      image,
      stream // æ–°å¢ stream è¾“å…¥
    } = inputs;

    const settings = api_settings as APISettings;
    const client = new OpenAI({
      baseURL: settings.base_url,
      apiKey: settings.api_key
    });

    const messages: Array<any> = [];

    // æ·»åŠ ç³»ç»Ÿæç¤º
    if (system_prompt) {
      messages.push({ role: 'system', content: system_prompt });
    }

    // æ·»åŠ å†å²è®°å½•
    if (history) {
      try {
        const historyMessages = JSON.parse(history);
        messages.push(...historyMessages);
      } catch (error) {
        const errorMsg = `è§£æå†å²è®°å½•å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`;
        console.warn(errorMsg);
        if (stream) {
          yield { type: 'error_chunk', content: errorMsg };
          return;
        } else {
          return { response: `Error: ${errorMsg}` };
        }
      }
    }
    
    // å¤„ç†å›¾ç‰‡å’Œæç¤ºä¿¡æ¯
    if (image) {
      try {
        const imageContent = await ImageProcessor.encodeImage(image);
        messages.push({
          role: 'user',
          content: [
            { type: 'text', text: prompt }, // å³ä½¿æœ‰å›¾ç‰‡ï¼Œæ–‡æœ¬æç¤ºä¹Ÿå¯èƒ½éœ€è¦
            {
              type: 'image_url',
              image_url: { url: `data:image/jpeg;base64,${imageContent}` }
            }
          ]
        });
      } catch (error) {
        const errorMsg = `å›¾åƒå¤„ç†å¤±è´¥ - ${error instanceof Error ? error.message : String(error)}`;
        if (stream) {
          yield { type: 'error_chunk', content: errorMsg };
          return;
        } else {
          return { response: `Error: ${errorMsg}` };
        }
      }
    } else {
      // å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œåˆ™å¿…é¡»æœ‰ prompt
      if (prompt) {
         messages.push({ role: 'user', content: prompt });
      } else if (messages.length === (system_prompt ? 1:0) ) {
        // å¦‚æœ messages æ•°ç»„åœ¨æ·»åŠ ç”¨æˆ·æ¶ˆæ¯å‰ï¼Œé•¿åº¦ç­‰äº (system_prompt ? 1:0)
        // è¿™æ„å‘³ç€æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿæ²¡æœ‰ç”¨æˆ·æ–‡æœ¬æç¤ºï¼Œå¹¶ä¸” messages åªåŒ…å«å¯èƒ½çš„ç³»ç»Ÿæç¤ºæˆ–ä¸ºç©º
        const errorMsg = "ç”¨æˆ·æç¤º (prompt) æˆ–å›¾åƒ (image) å¿…é¡»æä¾›è‡³å°‘ä¸€ä¸ª";
        if (stream) {
          yield { type: 'error_chunk', content: errorMsg };
          return;
        } else {
          return { response: `Error: ${errorMsg}` };
        }
      }
    }

    if (stream) {
      // --- æµå¼å¤„ç†é€»è¾‘ ---
      try {
        const streamResponse = await client.chat.completions.create({
          model,
          messages,
          temperature,
          max_tokens,
          stream: true, // å¯ç”¨æµå¼è¾“å‡º
        });

        for await (const part of streamResponse) {
          const content = part.choices[0]?.delta?.content || '';
          if (content) {
            const chunk: ChunkPayload = { type: 'text_chunk', content };
            yield chunk;
          }
          if (part.choices[0]?.finish_reason) {
             yield { type: 'finish_reason_chunk', content: part.choices[0]?.finish_reason };
          }
        }
        return; // æµå¼æ¨¡å¼ä¸‹è¿”å› void
      } catch (error) {
        const errorMsg = `OpenAI API æµå¼è¯·æ±‚é”™è¯¯: ${error instanceof Error ? error.message : String(error)}`;
        yield { type: 'error_chunk', content: errorMsg };
        return;
      }
    } else {
      // --- æ‰¹å¤„ç†é€»è¾‘ ---
      try {
        const responseData = await client.chat.completions.create({
          model,
          messages,
          temperature,
          max_tokens,
          stream: false,
        });
        return {
          response: responseData.choices[0]?.message?.content || ''
        };
      } catch (error) {
        return {
          response: `Error: OpenAI API æ‰¹å¤„ç†è¯·æ±‚é”™è¯¯ - ${error instanceof Error ? error.message : String(error)}`
        };
      }
    }
  }
}

// Renamed export to 'definition'
export const definition: NodeDefinition = {
  type: 'OpenAI', // Base type name
  // namespace will be set via index.ts registerer (e.g., 'builtin')
  category: 'TEST-LLM', // Functional category
  displayName: 'ğŸ¦‰OpenAIèŠå¤©',
  description: 'Generate text using OpenAI chat models',

  inputs: {
    api_settings: {
      dataFlowType: 'OBJECT', // API_SETTINGS maps to OBJECT
      displayName: 'API è®¾ç½®',
      description: 'æ¥è‡ª APISettings èŠ‚ç‚¹çš„ API è®¾ç½®',
      required: true,
      matchCategories: ['LlmConfig']
    },
    model: {
      dataFlowType: 'STRING',
      displayName: 'æ¨¡å‹',
      description: 'è¦ä½¿ç”¨çš„ OpenAI æ¨¡å‹',
      required: true,
      config: {
        default: 'gpt-4o',
        multiline: false,
        placeholder: 'è¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ gpt-4o, gemini-2.0-flash-exp ç­‰'
      }
    },
    temperature: {
      dataFlowType: 'FLOAT',
      displayName: 'æ¸©åº¦',
      description: 'é‡‡æ ·æ¸©åº¦',
      required: true,
      config: {
        default: 0.7,
        min: 0,
        max: 2,
        step: 0.1
      }
    },
    max_tokens: {
      dataFlowType: 'INTEGER',
      displayName: 'æœ€å¤§ä»¤ç‰Œæ•°',
      description: 'è¦ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°',
      required: true,
      config: {
        default: 512,
        min: 1,
        max: 16384
      }
    },
    system_prompt: {
      dataFlowType: 'STRING',
      displayName: 'ç³»ç»Ÿæç¤º',
      description: 'ç³»ç»Ÿæç¤º',
      required: true,
      matchCategories: ['Prompt'],
      config: {
        multiline: true
      }
    },
    prompt: {
      dataFlowType: 'STRING',
      displayName: 'ç”¨æˆ·æç¤º',
      description: 'ç”¨æˆ·æç¤º',
      required: true,
      matchCategories: ['Prompt'],
      config: {
        multiline: true
      }
    },
    history: {
      dataFlowType: 'STRING', // HISTORY input is a JSON string
      displayName: 'èŠå¤©è®°å½•',
      description: 'èŠå¤©è®°å½• (JSONå­—ç¬¦ä¸²)',
      required: false,
      matchCategories: ['ChatHistory', 'Json']
    },
    image: {
      dataFlowType: 'STRING', // IMAGE input is URL/Base64 string
      displayName: 'å›¾åƒ',
      description: 'è§†è§‰æ¨¡å‹çš„å›¾åƒ (URL/Base64)',
      required: false,
      matchCategories: ['ImageData', 'Url']
    },
    stream: { // æ–°å¢ stream è¾“å…¥æ§½
      dataFlowType: 'BOOLEAN',
      displayName: 'å¯ç”¨æµå¼è¾“å‡º',
      description: 'å¦‚æœä¸º trueï¼Œåˆ™å¯ç”¨æµå¼è¾“å‡ºï¼Œé€å—è¿”å›ç»“æœã€‚å¦åˆ™ï¼Œè¿›è¡Œæ‰¹å¤„ç†å¹¶ä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç»“æœã€‚',
      required: false,
      config: {
        default: false, // æ˜ç¡®é»˜è®¤å€¼ä¸º false
      },
    },
  },

  outputs: {
    response: {
      dataFlowType: 'STRING',
      displayName: 'ç”Ÿæˆå›å¤',
      description: 'ç”Ÿæˆçš„å›å¤',
      matchCategories: ['LlmOutput']
    }
  },

  execute: OpenAINodeImpl.execute
}

// Removed: Node registration is now handled by index.ts