import type { NodeDefinition, CustomMessage, StandardResponse, ApiCredentialConfig, ChunkPayload } from '@comfytavern/types';
import { LlmApiAdapterRegistry } from '../../services/LlmApiAdapterRegistry';
import { ApiConfigService } from '../../services/ApiConfigService';
import { ActivatedModelService } from '../../services/ActivatedModelService';

// Placeholder validation for a single message
function isValidCustomMessage(item: any): item is CustomMessage {
  return typeof item === 'object' && item !== null && 'role' in item && 'content' in item;
}

// Placeholder validation for an array of messages
function isValidCustomMessageArray(data: any): data is CustomMessage[] {
  if (!Array.isArray(data)) {
    return false;
  }
  return data.every(isValidCustomMessage);
}

class GenericLlmRequestNodeImpl {
  static async* execute(inputs: Record<string, any>, context: any): AsyncGenerator<ChunkPayload, Record<string, any> | void, undefined> {
    const {
      messages,
      parameters: baseParameters = {},
      activated_model_id,
      temperature,
      max_tokens,
      top_p,
      seed,
      stream = false, // æ–°å¢çš„æµå¼æ§åˆ¶å‚æ•°
    } = inputs;

    const { nodeId, services, userId } = context || {};

    // --- 1. Validate Context and Services ---
    if (!services) {
      throw new Error('Execution context is missing the "services" object. This is a critical error.');
    }
    const { apiConfigService, activatedModelService, llmApiAdapterRegistry } = services;
    if (!apiConfigService || !activatedModelService || !llmApiAdapterRegistry) {
      throw new Error('One or more required services (ApiConfigService, ActivatedModelService, LlmApiAdapterRegistry) are not available in the context.');
    }

    // --- 2. Validate Inputs ---
    if (!isValidCustomMessageArray(messages)) {
      throw new Error('Input "messages" is missing or not a valid CustomMessage array.');
    }
    if (typeof activated_model_id !== 'string' || !activated_model_id) {
      throw new Error('Input "activated_model_id" is missing or invalid.');
    }

    // channel_id is removed from inputs. The router service will handle channel selection.
    // For now, we'll fetch the first available channel as a placeholder.
    console.log(`[GenericLlmRequestNode ${nodeId}] Executing... Model: ${activated_model_id}`);

    // --- 3. Get Config ---
    // For MVP, we assume services are pre-initialized with a db connection and userId is handled by the service layer
    // TODO: Replace this with a call to a ModelRouterService that determines the appropriate channel.
    const allChannels = await apiConfigService.getAllCredentials(userId);
    // Sort by creation time to have a deterministic order
    allChannels.sort((a: ApiCredentialConfig, b: ApiCredentialConfig) => new Date(a.createdAt || 0).getTime() - new Date(b.createdAt || 0).getTime());

    const channelConfig = allChannels.find((c: ApiCredentialConfig) =>
      !c.disabled &&
      (!c.supportedModels || c.supportedModels.length === 0 || c.supportedModels.includes(activated_model_id))
    );

    if (!channelConfig) {
      throw new Error(`No active channel found for user that supports model "${activated_model_id}".`);
    }
    console.log(`[GenericLlmRequestNode ${nodeId}] Using channel: ${channelConfig.label} (${channelConfig.id})`);

    // TODO: æ¿€æ´»æ¨¡å‹ç®¡ç†åŠŸèƒ½å®Œæˆåï¼Œéœ€è¦æ¢å¤æ­¤å¤„çš„æ£€æŸ¥
    // We don't strictly need model info for the request itself in MVP, but it's good practice to check.
    // const modelInfo = await activatedModelService.getActivatedModel(activated_model_id);
    // if (!modelInfo) {
    //   throw new Error(`Activated model "${activated_model_id}" not found.`);
    // }

    // --- 4. Get Adapter and Execute Request ---
    const adapterType = channelConfig.adapterType || 'openai'; // Default to openai for MVP
    const adapter = llmApiAdapterRegistry.getAdapter(adapterType);

    if (!adapter) {
      throw new Error(`LLM adapter for type "${adapterType}" not found.`);
    }

    // --- 5. Prepare Parameters ---
    // Start with individual parameters from the node's UI.
    const individualParameters: Record<string, any> = {};
    if (temperature !== undefined) individualParameters.temperature = temperature;
    if (max_tokens !== undefined) individualParameters.max_tokens = max_tokens;
    if (top_p !== undefined) individualParameters.top_p = top_p;
    // Only add seed if it's a positive integer and not 0, as 0 can sometimes be ignored.
    if (seed !== undefined && seed > 0) individualParameters.seed = seed;

    // Merge with the parameters from the input slot.
    // The input slot (`baseParameters`) will override the individual settings.
    const finalParameters = { ...individualParameters, ...baseParameters };

    try {
      const payload = {
        messages,
        modelConfig: {
          model: activated_model_id,
          ...finalParameters,
        },
        channelConfig: {
          baseUrl: channelConfig.baseUrl,
          apiKey: channelConfig.apiKey,
        },
        stream: stream,
      };

      if (stream) {
        // æµå¼æ¨¡å¼
        let accumulatedText = '';
        let finalResponse: StandardResponse | null = null;

        for await (const chunk of adapter.executeStream(payload)) {
          // å°†æ¯ä¸ªæµå—å‘é€ç»™å‰ç«¯
          yield chunk;

          // ç´¯ç§¯æ–‡æœ¬å†…å®¹
          if (chunk.type === 'text_chunk' && typeof chunk.content === 'string') {
            accumulatedText += chunk.content;
          }

          // å¤„ç†å®Œæˆå—
          if (chunk.type === 'finish_reason_chunk') {
            finalResponse = {
              text: chunk.content?.accumulated_text || accumulatedText,
              model: activated_model_id,
            };
          }
        }

        // åœ¨æµç»“æŸåè¿”å›æœ€ç»ˆç»“æœ
        return {
          response: finalResponse,
          responseText: accumulatedText,
        };

      } else {
        // éæµå¼æ¨¡å¼
        const response = await adapter.execute(payload) as StandardResponse;

        if (response.error) {
          throw new Error(`LLM API Error: ${response.error.message}`);
        }

        console.log(`[GenericLlmRequestNode ${nodeId}] Execution successful. Model used: ${response.model}`);

        return {
          response: response,
          responseText: response.text,
        };
      }

    } catch (error: any) {
      console.error(`[GenericLlmRequestNode ${nodeId}] Error during LLM request execution: ${error.message}`);
      throw error; // Re-throw to be caught by the execution engine
    }
  }
}

export const genericLlmRequestNodeDefinition: NodeDefinition = {
  type: 'GenericLlmRequest',
  category: 'LLM',
  displayName: 'âš¡é€šç”¨ LLM è¯·æ±‚',
  description: 'å‘æŒ‡å®šçš„ LLM æ¨¡å‹å‘é€è¯·æ±‚ï¼Œæ¸ é“ç”±åç«¯è‡ªåŠ¨è·¯ç”±',
  width: 350,

  inputs: {
    messages: {
      dataFlowType: 'ARRAY',
      displayName: 'æ¶ˆæ¯åˆ—è¡¨',
      description: 'è¦å‘é€ç»™ LLM çš„æ¶ˆæ¯æ•°ç»„ (CustomMessage[])\n\nç¤ºä¾‹æ ¼å¼:\n```json\n[\n  {\n    "role": "user",\n    "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"\n  },\n  {\n    "role": "assistant",\n    "content": "æˆ‘æ˜¯ComfyTavernåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡"\n  }\n]\n```',
      matchCategories: ['ChatHistory','NoDefaultEdit'],
    },
    parameters: {
      dataFlowType: 'OBJECT',
      displayName: 'å‚æ•°',
      description: 'ä¼ é€’ç»™ LLM API çš„å‚æ•° (e.g., temperature, max_tokens) \nä¼šè¦†ç›–æœ¬èŠ‚ç‚¹ UI ä¸­çš„å•ç‹¬è®¾ç½®',
      required: false,
      matchCategories: ['LlmConfig','NoDefaultEdit'],
      config: {
        default: {},
      },
    },
    activated_model_id: {
      dataFlowType: 'STRING',
      displayName: 'æ¨¡å‹ ID',
      description: 'è¦ä½¿ç”¨çš„å·²æ¿€æ´»æ¨¡å‹çš„ ID \næ¯”å¦‚`claude-opus-4-20250522`ã€`gemini-2.5-pro`ã€`deepseek-reasoner`ç­‰',
    },
    temperature: {
      dataFlowType: 'FLOAT',
      displayName: 'æ¸©åº¦',
      description: ' (Temperature) æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚è¾ƒé«˜çš„å€¼ä¼šä½¿è¾“å‡ºæ›´éšæœºã€‚',
      required: false,
      config: {
        default: 0.7,
        min: 0.0,
        max: 2.0,
        step: 0.01,
      }
    },
    max_tokens: {
      dataFlowType: 'INTEGER',
      displayName: 'æœ€å¤§ä»¤ç‰Œæ•°',
      description: ' (Max Tokens) ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°ã€‚',
      required: false,
      config: {
        default: 2048,
        min: 1,
        max: 65536,
        step: 64,
      }
    },
    top_p: {
      dataFlowType: 'FLOAT',
      displayName: 'Top P',
      description: 'æ ¸å¿ƒé‡‡æ ·ã€‚æ¨¡å‹ä¼šè€ƒè™‘æ¦‚ç‡è´¨é‡ä¸º top_p çš„ä»¤ç‰Œã€‚0.1 æ„å‘³ç€åªè€ƒè™‘æ„æˆå‰ 10% æ¦‚ç‡è´¨é‡çš„ä»¤ç‰Œã€‚',
      required: false,
      config: {
        default: 1.0,
        min: 0.0,
        max: 1.0,
        step: 0.01,
      }
    },
    seed: {
      dataFlowType: 'INTEGER',
      displayName: 'éšæœºç§å­',
      description: ' (Seed) ç”¨äºå¯å¤ç°è¾“å‡ºçš„éšæœºç§å­ã€‚',
      required: false,
      config: {
        default: 0,
        min: 0,
        step: 1,
      }
    },
    stream: {
      dataFlowType: 'BOOLEAN',
      displayName: 'å¯ç”¨æµå¼è¾“å‡º',
      description: 'å¦‚æœä¸º trueï¼Œå°†ä»¥æµçš„å½¢å¼è¾“å‡ºç»“æœï¼Œå¦åˆ™åœ¨å®Œæˆåä¸€æ¬¡æ€§è¾“å‡ºã€‚',
      required: false,
      config: {
        default: false,
      },
    },
  },
  outputs: {
    response: {
      dataFlowType: 'OBJECT',
      displayName: 'å®Œæ•´å“åº”',
      description: 'LLM è¿”å›çš„å®Œæ•´æ ‡å‡†åŒ–å“åº”å¯¹è±¡ã€‚\nåœ¨æµå¼æ¨¡å¼ä¸‹ï¼Œæ­¤è¾“å‡ºåœ¨æµç»“æŸåæ‰å¯ç”¨ã€‚',
      matchCategories: ['LlmOutput']
    },
    responseText: {
      dataFlowType: 'STRING',
      displayName: 'å“åº”æ–‡æœ¬',
      description: 'LLM è¿”å›çš„ä¸»è¦æ–‡æœ¬å†…å®¹ã€‚\nåœ¨æµå¼æ¨¡å¼ä¸‹ï¼Œè¿™æ˜¯ç´¯ç§¯æ‰€æœ‰æ–‡æœ¬å—åçš„æœ€ç»ˆç»“æœã€‚',
      matchCategories: ['LlmOutput']
    },
    stream_output: {
      dataFlowType: 'STRING',
      isStream: true,
      displayName: 'æµå¼è¾“å‡º',
      description: 'å½“å¯ç”¨æµå¼è¾“å‡ºæ—¶ï¼Œä»è¿™é‡Œé€å—è¾“å‡ºç»“æœã€‚',
      matchCategories: ['StreamChunk']
    }
  },

  // MVP version uses direct inputs, so no config schema is needed for now.
  // The 'required_capabilities' logic will be added in a later phase.
  configSchema: {},

  execute: GenericLlmRequestNodeImpl.execute,
};

// --- èŠ‚ç‚¹2: åˆ›å»ºå•æ¡æ¶ˆæ¯ ---
class CreateMessageNodeImpl {
  static async execute(inputs: Record<string, any>): Promise<Record<string, any>> {
    const { role = 'user', content = '' } = inputs;
    if (!content) {
      // å¦‚æœå†…å®¹ä¸ºç©ºï¼Œè¿”å›ä¸€ä¸ªç©ºæ•°ç»„
      return { messages: [] };
    }
    // ç›´æ¥è¿”å›åŒ…å«å•æ¡æ¶ˆæ¯çš„æ•°ç»„
    return {
      messages: [{ role, content }],
    };
  }
}

export const createMessageNodeDefinition: NodeDefinition = {
  type: 'CreateMessage',
  category: 'LLM',
  displayName: 'ğŸ’¬åˆ›å»ºæ¶ˆæ¯',
  description: 'åˆ›å»ºä¸€æ¡å•ç‹¬çš„å¯¹è¯æ¶ˆæ¯',
  width: 300,
  inputs: {
    role: {
      dataFlowType: 'STRING',
      displayName: 'è§’è‰²',
      description: 'æ¶ˆæ¯å‘é€è€…çš„è§’è‰²',
      required: true,
      matchCategories: ['ComboOption'],
      config: {
        default: 'user',
        suggestions: [
          { value: 'system', label: 'system' },
          { value: 'user', label: 'user' },
          { value: 'assistant', label: 'assistant' }
        ],
      },
    },
    content: {
      dataFlowType: 'STRING',
      displayName: 'å†…å®¹',
      description: 'æ¶ˆæ¯çš„å…·ä½“å†…å®¹',
      required: true,
      matchCategories: ['UiBlock', 'CanPreview'],
      config: {
        default: '',
        multiline: true,
        placeholder: 'è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹...',
      },
    },
  },
  outputs: {
    messages: {
      dataFlowType: 'ARRAY',
      displayName: 'æ¶ˆæ¯åˆ—è¡¨',
      description: 'åŒ…å«å•æ¡æ¶ˆæ¯çš„æ¶ˆæ¯åˆ—è¡¨',
      matchCategories: ['ChatHistory'],
    },
  },
  execute: CreateMessageNodeImpl.execute,
};


// --- èŠ‚ç‚¹3: åˆå¹¶å¤šæ¡æ¶ˆæ¯ ---
class MergeMessagesNodeImpl {
  static async execute(inputs: Record<string, any>): Promise<Record<string, any>> {
    const { message_inputs = [] } = inputs;

    const flattenedMessages: CustomMessage[] = [];

    // ç¡®ä¿è¾“å…¥æ˜¯æ•°ç»„
    const values = Array.isArray(message_inputs) ? message_inputs : [message_inputs];

    for (const item of values) {
      if (!item) continue; // è·³è¿‡ null æˆ– undefined

      if (Array.isArray(item)) {
        // å¦‚æœæ˜¯æ•°ç»„ (æ¥è‡ªå¦ä¸€ä¸ªåˆå¹¶èŠ‚ç‚¹æˆ–å†å²è®°å½•), åˆ™å±•å¼€å¹¶è¿‡æ»¤
        flattenedMessages.push(...item.filter(isValidCustomMessage));
      } else if (isValidCustomMessage(item)) {
        // å¦‚æœæ˜¯å•æ¡æ¶ˆæ¯å¯¹è±¡
        flattenedMessages.push(item);
      }
    }

    return {
      messages: flattenedMessages,
    };
  }
}

export const mergeMessagesNodeDefinition: NodeDefinition = {
  type: 'MergeMessages',
  category: 'LLM',
  displayName: 'ğŸ¤åˆå¹¶æ¶ˆæ¯',
  description: 'å°†å¤šæ¡æ¶ˆæ¯æˆ–æ¶ˆæ¯åˆ—è¡¨åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„æ¶ˆæ¯å†å²',
  width: 200,
  inputs: {
    message_inputs: {
      dataFlowType: 'OBJECT', // æ¥å—å•æ¡æ¶ˆæ¯
      displayName: 'æ¶ˆæ¯è¾“å…¥',
      description: 'è¦åˆå¹¶çš„æ¶ˆæ¯æˆ–æ¶ˆæ¯åˆ—è¡¨',
      required: true,
      multi: true,
      // å…è®¸è¿æ¥å•æ¡æ¶ˆæ¯æˆ–å®Œæ•´çš„æ¶ˆæ¯å†å²è®°å½•
      matchCategories: ['ChatMessage', 'ChatHistory'],
    },
  },
  outputs: {
    messages: {
      dataFlowType: 'ARRAY',
      displayName: 'æ¶ˆæ¯åˆ—è¡¨',
      description: 'åˆå¹¶åçš„æ¶ˆæ¯åˆ—è¡¨',
      matchCategories: ['ChatHistory'],
    },
  },
  execute: MergeMessagesNodeImpl.execute,
};